# YouTube Transcriber & Translator

A flexible toolkit for transcribing and translating YouTube videos, audio files, and existing documents.

## üéØ Highlights

### Version 1.5 (current)
- ‚úÖ **Speaker Diarization**
  - Automatic speaker identification using pyannote.audio
  - Speaker labels in transcripts ([SPEAKER_00], [SPEAKER_01], etc.)
  - Works with both local Whisper and OpenAI API
  - Optimal speaker detection using VAD integration
  - Enable with `--speakers` flag
  - ‚ö†Ô∏è Note: May over-segment speakers (one person ‚Üí multiple labels); manual review recommended for critical use
- ‚úÖ **Enhanced logging with colored output**
  - Color-coded log levels for better visibility
  - WARNING messages in orange for important notices
  - INFO messages in green for successful operations
  - ERROR/CRITICAL messages in red for failures
  - Smart warnings (e.g., missing Whisper prompt suggestions)

### Version 1.4
- ‚úÖ **Video file support**
  - Process local video files (MP4, MKV, AVI, MOV, etc.)
  - Automatic audio extraction using FFmpeg
  - Full pipeline support (transcribe, translate, refine)

### Version 1.3
- ‚úÖ **Document processing** (.docx, .md, .txt, .pdf)
  - Read existing transcripts
  - **PDF support**
  - Post-process text with an LLM
  - Translate uploaded documents
  - Automatic language detection
- ‚úÖ **Quality & testing**
  - 139 automated tests with 49% coverage
  - CI/CD powered by GitHub Actions
  - Pre-commit hooks (black, flake8, mypy)
  - Full type hints across the codebase

### Version 1.1
- ‚úÖ **Optimised prompts for LLM polishing**
  - Removes filler words ("um", "uh", etc.)
  - Normalises numbers ("twenty eight" ‚Üí "28")
  - Preserves **all** facts and examples
  - Works for both Russian and English content

### Version 1.0
- ‚úÖ Downloading and processing YouTube videos
- ‚úÖ Processing local audio files (mp3, wav, ...)
- ‚úÖ Processing local video files (mp4, mkv, avi, ...)
- ‚úÖ Whisper-based transcription (base, small, medium)
- ‚úÖ LLM-based refinement through Ollama (qwen2.5, llama3, ...)
- ‚úÖ Automatic language detection (ru/en)
- ‚úÖ Translation with Meta NLLB
- ‚úÖ Export to .docx and .md
- ‚úÖ Custom Whisper prompts (from file)
- ‚úÖ Prompt generation from YouTube metadata
- ‚úÖ Rich logging and progress bars
- ‚úÖ Apple M1/M2 optimisations

### In progress
- üîÑ Optimized chunk processing for OpenAI API
- üîÑ Batch processing support
- üîÑ Docker support

## üìã Requirements

### System
- Python 3.9+
- FFmpeg (audio preprocessing)
- Ollama (optional, for LLM refinement)
- 8 GB RAM minimum, 16 GB recommended
- ~5 GB disk space for Whisper and NLLB models
- Additional 3‚Äì7 GB if you use Ollama models

### Supported platforms
- macOS (including Apple Silicon)
- Linux
- Windows

## üöÄ Installation

### 1. Clone the repository

```bash
git clone <repository-url>
cd yt-transcriber
```

### 2. Create a virtual environment

```bash
python -m venv venv

# macOS/Linux
source venv/bin/activate

# Windows
venv\Scripts\activate
```

### 3. Install FFmpeg

**macOS**
```bash
brew install ffmpeg
```

**Linux (Ubuntu/Debian)**
```bash
sudo apt update
sudo apt install ffmpeg
```

**Windows**
Download a build from [ffmpeg.org](https://ffmpeg.org/download.html) and add it to your `PATH`.

### 4. Install Python dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 5. Install Ollama (optional, for refinement)

**macOS/Linux**
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Recommended models
ollama pull qwen2.5:3b    # Fast, good quality (~3 GB)
ollama pull qwen2.5:7b    # Slower, higher quality (~7 GB)

# Start the server (if not already running)
ollama serve
```

**Windows**
Download the installer from [ollama.com](https://ollama.com/download).

### 6. Environment variables (optional)

Create a `.env` file in the project root:

```bash
# Enable OpenAI integration (experimental)
OPENAI_API_KEY=your_api_key_here

# Logging level
LOG_LEVEL=INFO
```

## üìñ Usage

### Quick examples

#### 1. Transcribe a YouTube video

```bash
python -m src.main --url "https://youtube.com/watch?v=dQw4w9WgXcQ" --transcribe whisper_base
```

#### 2. Transcribe and translate

```bash
python -m src.main     --url "https://youtube.com/watch?v=dQw4w9WgXcQ"     --transcribe whisper_base     --translate NLLB
```

#### 3. Process a local audio file

```bash
python -m src.main     --input_audio audio.mp3     --transcribe whisper_medium     --translate NLLB
```

#### 4. Process a local video file

```bash
python -m src.main     --input_video video.mp4     --transcribe whisper_medium     --translate NLLB
```

Supported video formats: MP4, MKV, AVI, MOV, and any format supported by FFmpeg.

#### 5. Refine a transcript with an LLM

```bash
python -m src.main     --input_audio audio.mp3     --transcribe whisper_medium     --refine-model qwen2.5:7b     --translate NLLB
```

Produces two documents:
- `audio (original).docx/md` ‚Äî raw transcript without translation
- `audio (refined).docx/md` ‚Äî polished transcript with translation

#### 6. Use a custom Whisper prompt

```bash
# Create prompt.txt with project-specific terms
# FIDE, Hikaru Nakamura, Magnus Carlsen, chess tournament

python -m src.main     --url "https://youtube.com/watch?v=YOUR_VIDEO_ID"     --transcribe whisper_base     --prompt prompt.txt
```

#### 7. Enable speaker diarization (v1.5)

```bash
# Transcribe with automatic speaker identification
python -m src.main \
    --url "https://youtube.com/watch?v=YOUR_VIDEO_ID" \
    --transcribe whisper_medium \
    --speakers
```

**Requirements for speaker diarization:**
1. Get HuggingFace token: https://huggingface.co/settings/tokens (create a "Read" token)
2. Accept model terms for all required models:
   - https://huggingface.co/pyannote/speaker-diarization-3.1
   - https://huggingface.co/pyannote/segmentation-3.0
   - https://huggingface.co/pyannote/speaker-diarization-community-1
   - https://huggingface.co/pyannote/voice-activity-detection (optional, for better chunking)
3. Set token in environment: `export HF_TOKEN=your_token_here` (add to `~/.zshrc` or `~/.bashrc`)

Output will include speaker labels:
```
[00:00] [SPEAKER_00] Hello everyone, welcome to the show
[00:05] [SPEAKER_01] Thanks for having me
[00:08] [SPEAKER_00] Let's get started with today's topic
```

## ‚öñÔ∏è Legal notice
- Make sure you respect YouTube Terms of Service and copyright law before downloading or processing any content. Only use the tool for media you own or have explicit permission to process.
- Output documents and logs may contain fragments of the original content. Store them locally and review licences before sharing.
- The default translation model `facebook/nllb-200-distilled-1.3B` is released under CC BY-NC 4.0 (non-commercial). Use a different model or obtain a licence for commercial scenarios.

#### 7. Process existing documents (v1.2)

```bash
# Improve an existing transcript
python -m src.main     --input_text output/document.md     --refine-model qwen2.5:7b

# Translate a document
python -m src.main     --input_text transcription.docx     --translate NLLB

# Refine and translate
python -m src.main     --input_text document.txt     --refine-model qwen2.5:7b     --translate NLLB
```

Supported formats: `.md`, `.docx`, `.txt`

#### 8. Help screen

```bash
python -m src.main --help
```

### CLI arguments

| Argument | Description | Example |
|----------|-------------|---------|
| `--url` | YouTube video URL | `--url "https://youtube.com/..."` |
| `--input_audio` | Path to an audio file (mp3, wav, ‚Ä¶) | `--input_audio audio.mp3` |
| `--input_video` | Path to a video file (mp4, mkv, avi, ‚Ä¶) | `--input_video video.mp4` |
| `--input_text` | Path to a text document (.docx, .md, .txt) | `--input_text doc.docx` |
| `--transcribe` | Transcription backend | `--transcribe whisper_medium` |
| `--translate` | Translation backend | `--translate NLLB` |
| `--refine-model` | Ollama model for refinement | `--refine-model qwen2.5:7b` |
| `--prompt` | Custom Whisper prompt file | `--prompt prompt.txt` |
| `--speakers` | Enable speaker diarisation (experimental) | `--speakers` |
| `--help` | Show help | `--help` |

### Available methods

**Transcription**
- `whisper_base` ‚Äî fast, good quality
- `whisper_small` ‚Äî slower, higher quality
- `whisper_medium` ‚Äî slowest, best quality
- `whisper_openai_api` ‚Äî OpenAI Whisper (coming soon)

**Refinement (requires Ollama)**
- `qwen2.5:3b` ‚Äî fast, 3 GB (recommended)
- `qwen2.5:7b` ‚Äî slower, better quality
- `llama3.2:3b` ‚Äî fast, solid quality
- `llama3:8b` ‚Äî slower, higher quality
- `mistral:7b` ‚Äî balanced
- Any other model available in the [Ollama library](https://ollama.com/library)

**Translation**
- `NLLB` ‚Äî Meta NLLB (local, free)
- `openai_api` ‚Äî OpenAI API (coming soon)

## üìÅ Project structure

```
yt-transcriber/
‚îú‚îÄ‚îÄ src/                      # Source code
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Entry point
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ downloader.py        # YouTube downloads
‚îÇ   ‚îú‚îÄ‚îÄ transcriber.py       # Transcription
‚îÇ   ‚îú‚îÄ‚îÄ text_reader.py       # Text ingestion
‚îÇ   ‚îú‚îÄ‚îÄ translator.py        # Translation
‚îÇ   ‚îú‚îÄ‚îÄ text_refiner.py      # LLM-based refinement
‚îÇ   ‚îú‚îÄ‚îÄ document_writer.py   # Document generation
‚îÇ   ‚îú‚îÄ‚îÄ utils.py             # Utilities
‚îÇ   ‚îî‚îÄ‚îÄ logger.py            # Logging setup
‚îú‚îÄ‚îÄ tests/                   # Automated tests
‚îú‚îÄ‚îÄ output/                  # Generated docs
‚îú‚îÄ‚îÄ temp/                    # Temporary files
‚îú‚îÄ‚îÄ logs/                    # Logs
‚îú‚îÄ‚îÄ requirements.txt         # Runtime dependencies
‚îú‚îÄ‚îÄ .env.example             # Sample configuration
‚îî‚îÄ‚îÄ README.md                # Documentation
```

**Note:** Whisper and NLLB models are cached in `~/.cache/` on first run.

## üîß Configuration

Main settings live in `src/config.py`:

```python
# Paths
OUTPUT_DIR = "output"        # Output folder
TEMP_DIR = "temp"            # Temporary files
LOGS_DIR = "logs"            # Logs

# Models
WHISPER_DEVICE = "mps"       # cpu/cuda/mps (auto-switch for M1)
NLLB_MODEL_NAME = "facebook/nllb-200-distilled-600M"

# Logging
LOG_LEVEL = "INFO"           # DEBUG/INFO/WARNING/ERROR
```

## üìä Performance

Approximate processing time on a MacBook Air M1 (16 GB, CPU):

| Video length | whisper_base | whisper_small | NLLB translation | Total (base+translate) | Total (small+translate) |
|--------------|--------------|---------------|------------------|------------------------|-------------------------|
| 3 minutes    | ~11 s        | ~34 s         | ~1.5 min         | ~2 min                 | ~3 min                  |
| 10 minutes   | ~36 s        | ~2 min        | ~5 min           | ~5.5 min               | ~7 min                  |
| 30 minutes   | ~1.8 min     | ~5.7 min      | ~14 min          | ~16 min                | ~20 min                 |
| 1 hour       | ~3.6 min     | ~11 min       | ~28 min          | ~32 min                | ~39 min                 |
| 2 hours      | ~7 min       | ~23 min       | ~56 min          | ~63 min                | ~79 min                 |

**Processing factors:**
- Whisper Base: 0.06√ó (‚âà16√ó faster than realtime) üöÄ
- Whisper Small: 0.19√ó (‚âà5√ó faster than realtime)
- NLLB: 0.47√ó (‚âà2√ó faster than realtime)

## üêõ Troubleshooting

### Installation issues

**Problem:** `torch` fails to install on Apple Silicon
```bash
# Use the dedicated Apple Silicon build
pip install --upgrade torch torchvision torchaudio
```

**Problem:** FFmpeg not found
```bash
ffmpeg -version
# If missing, install via Homebrew (macOS)
brew install ffmpeg
```

**Problem:** Out of memory
```bash
# Switch to a smaller Whisper model
python -m src.main --url "..." --transcribe whisper_base
```

### Runtime issues

**Problem:** `Model not found`
- Models download automatically on first run
- Ensure you have an internet connection
- Check that the `models/` directory is writable

**Problem:** Processing is slow
- Use `whisper_base` instead of `whisper_small`
- Confirm that GPU/MPS acceleration is active (see logs)
- Close other resource-heavy applications

**Safe to ignore:** Speaker diarization warnings
- `UserWarning: torchcodec is not installed correctly` ‚Äî Audio loading uses soundfile/librosa fallback (works correctly)
- `UserWarning: std(): degrees of freedom is <= 0` ‚Äî Internal pyannote calculation (does not affect results)
- See [FAQ.md](FAQ.md) for detailed explanations

## üß™ Testing

```bash
# Install dev dependencies
pip install -r requirements-dev.txt

# Run tests
pytest tests/

# Coverage report
pytest --cov=src tests/
```

## üìù Sample output

### .docx format
```
# Video title

## Translation
Method: NLLB

[00:15] Hello everyone! Today we will talk about...

[01:32] The first important topic is...

## Transcript
Method: whisper_base

[00:15] Hello everyone! Today we'll talk about...

[01:32] The first important topic is...
```

### .md format
Uses the same layout with Markdown syntax.

## üõ£Ô∏è Roadmap

### v1.0 ‚Äî ‚úÖ Shipped
- ‚úÖ YouTube + local audio ingestion
- ‚úÖ Whisper (base, small, medium)
- ‚úÖ LLM-based refinement via Ollama
- ‚úÖ NLLB translation
- ‚úÖ Custom prompts
- ‚úÖ Automatic language detection

### v2.0 ‚Äî Planned
- [ ] Extended document ingestion
- [ ] OpenAI API integration
- [ ] Speaker diarisation
- [ ] Enhanced CI/CD + unit tests
- [ ] Docker image
- [ ] Web UI
- [ ] Batch processing helpers

## ü§ù Contributing

Pull requests are welcome! For major changes, open an issue first to discuss what you would like to improve.

### Development flow

1. Fork the repository
2. Create a branch (`git checkout -b feature/amazing-feature`)
3. Commit (`git commit -m 'Add amazing feature'`)
4. Push (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## üìÑ License

- Distributed under the MIT License ‚Äî see `LICENSE` for details.
- The codebase was developed with help from AI-assisted tools (e.g., GitHub Copilot, Codex). All code and docs were reviewed and validated manually before publishing.

## üôè Acknowledgements

- [OpenAI Whisper](https://github.com/openai/whisper) ‚Äî transcription
- [Meta NLLB](https://github.com/facebookresearch/fairseq/tree/nllb) ‚Äî translation
- [Ollama](https://ollama.com) ‚Äî local LLMs
- [yt-dlp](https://github.com/yt-dlp/yt-dlp) ‚Äî YouTube downloads

## üìû Contact

For questions or suggestions, please open an issue in this repository.

---

## üí° Usage tips

### Improve transcription quality
1. Use `whisper_medium` for critical content
2. Provide prompt files with key terms and names
3. For YouTube sources, metadata-derived prompts are added automatically

### Improve text quality
1. Install Ollama and pull `qwen2.5:7b` for best results
2. Language detection switches between Russian and English automatically
3. Use `--refine-model` to produce a clean transcript
4. **New in v1.1:** the LLM prompt
   - Removes filler words ("um", "uh", "—ç–º", "–Ω—É", "–≤–æ—Ç")
   - Skips meta commentary ("let me scroll", "—Å–µ–π—á–∞—Å –æ—Ç–∫—Ä–æ—é —ç–∫—Ä–∞–Ω")
   - Normalises numbers ("twenty eight sixteen" ‚Üí "2816", "–Ω–æ–ª—å –≤–æ—Å–µ–º—å" ‚Üí "0.8")
   - **Keeps every detail**: examples, facts, reasoning
   - No summarisation ‚Äî only clean-up and structuring
   - Fixes punctuation and paragraphing

### Optimise speed
- `whisper_base` ‚Äî high throughput
- `whisper_medium` ‚Äî best accuracy
- `qwen2.5:3b` ‚Äî fast refinement
- `qwen2.5:7b` ‚Äî highest quality

### Model cache locations
- Whisper: `~/.cache/whisper/` (~140 MB ‚Äì 1.5 GB)
- Ollama: manage via `ollama list` and `ollama rm <model>`
